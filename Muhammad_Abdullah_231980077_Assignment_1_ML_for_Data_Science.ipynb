{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPRnlJeqrLeo5C95wfaPyTr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaudhayabdullah786/Billing-Management-System/blob/main/Muhammad_Abdullah_231980077_Assignment_1_ML_for_Data_Science.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " December 14,2025   \n",
        "\n",
        " Machine Learning FOr Data\n",
        " Science  \n",
        "\n",
        "Sunday                                                                 Assignment 1\n",
        "\n",
        "Muhammad Abdullah\n",
        "\n",
        "231980077\n",
        "\n",
        "Shawaiz Ali\n",
        "\n",
        "231980079 **bold text**\n",
        "\n",
        "Question#1\n",
        "To prepare the raw data for k-NN classification,. Each file’s data was divided into segments of\n",
        "1000 samples, and labels were assigned based on the motor condition (e.g., healthy or inner-race\n",
        "fault). I then combined all the processed data into a single CSV file. Missing values were handled\n",
        "by removing rows with null entries. As a result, I obtained a clean dataset containing 5168 rows\n",
        "and 1001 columns, with 1000 feature columns and 1 label column.\n",
        "The code does the following:\n",
        "1. Explains its purpose.\n",
        "2. Details the key steps involved.\n",
        "# 3. Summarizes the output or final result. **bold text**"
      ],
      "metadata": {
        "id": "7DcX8zBk0MHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Change folder_path to a Colab accessible location (e.g., /content/)\n",
        "# You need to upload your CSV files to this location or mount Google Drive.\n",
        "folder_path = Path(r\"/content/\") # Assuming files are uploaded directly to /content/\n",
        "\n",
        "# Create an empty list to store DataFrames for each file\n",
        "output_dfs = []\n",
        "# Loop through all CSV files in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        # Construct the full file path\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        print(f\"Processing file: {file_path}\") # Added print statement to show the file being processed\n",
        "        # Extract the file name without extension\n",
        "        file_name_without_extension = file_name.split(\"-\")[0]\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(file_path)\n"
      ],
      "metadata": {
        "id": "Is9mpoIG0ccD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Folder containing CSVs\n",
        "# IMPORTANT: Change this to a path accessible in Google Colab (e.g., /content/ or a mounted Google Drive path)\n",
        "folder_path = Path(r\"/content/\") # Example: Assuming your CSV files are uploaded to /content/\n",
        "\n",
        "output_dfs = []\n",
        "\n",
        "# Loop through CSV files\n",
        "for file_path in folder_path.glob(\"*.csv\"):\n",
        "    # Skip generated output files to prevent errors\n",
        "    if file_path.name == \"output.csv\" or file_path.name == \"distance_matrix.csv\":\n",
        "        print(f\"Skipping generated file: {file_path.name}\")\n",
        "        continue\n",
        "\n",
        "    print(\"Processing:\", file_path.name)\n",
        "    df = pd.read_csv(file_path, low_memory=False, on_bad_lines='skip')\n",
        "\n",
        "    file_name_without_extension = file_path.stem\n",
        "\n",
        "    # Extract 'Current-A' column (strip leading spaces)\n",
        "    column_name_candidates = [col for col in df.columns if col.strip() == 'Current-A']\n",
        "    if not column_name_candidates:\n",
        "        print(f\"Warning: ' Current-A' column not found in {file_path.name}. Skipping file.\")\n",
        "        continue\n",
        "    column_name = column_name_candidates[0]\n",
        "    current_a_data = df[column_name]\n",
        "\n",
        "    # Chunk size\n",
        "    chunk_size = 1000\n",
        "\n",
        "    # Number of complete chunks\n",
        "    num_complete_chunks = len(current_a_data) // chunk_size\n",
        "\n",
        "    # Lists to store chunks and labels\n",
        "    transposed_chunks_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    # Process chunks\n",
        "    for i in range(0, num_complete_chunks * chunk_size, chunk_size):\n",
        "        chunk = current_a_data[i:i + chunk_size]\n",
        "\n",
        "        # Transpose chunk\n",
        "        transposed_chunk = chunk.values.reshape(1, -1)\n",
        "\n",
        "        # Append chunk and label\n",
        "        transposed_chunks_list.append(transposed_chunk)\n",
        "        labels_list.append(file_name_without_extension)\n",
        "\n",
        "    # Concatenate chunks\n",
        "    output_array = np.concatenate(transposed_chunks_list, axis=0)\n",
        "\n",
        "    # Create DataFrame\n",
        "    output_df = pd.DataFrame(\n",
        "        data=output_array,\n",
        "        columns=[f'Value_{j}' for j in range(output_array.shape[1])]\n",
        "    )\n",
        "\n",
        "    # Add label column\n",
        "    output_df['Label'] = labels_list\n",
        "\n",
        "    # Append to main list\n",
        "    output_dfs.append(output_df)\n",
        "\n",
        "# Combine all CSVs\n",
        "final_output_df = pd.concat(output_dfs, axis=0, ignore_index=True)\n",
        "\n",
        "# Save final CSV\n",
        "final_output_file_path = folder_path / \"output.csv\"\n",
        "final_output_df.to_csv(final_output_file_path, index=False)\n",
        "\n",
        "print(\"Saved final output to:\", final_output_file_path)\n"
      ],
      "metadata": {
        "id": "FRydJJub0RZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a message to indicate that the final file has been saved\n",
        "print('Final transposed data with labels saved to {}'.\n",
        " format(final_output_file_path))"
      ],
      "metadata": {
        "id": "N2HnyzF62CVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Full path to your output CSV\n",
        "# Update this to the correct path in Colab, which is /content/output.csv\n",
        "output_file_path = Path(r\"/content/output.csv\")\n",
        "\n",
        "# Read CSV\n",
        "df = pd.read_csv(output_file_path)\n",
        "\n",
        "# Check for null values\n",
        "null_values = df.isnull().sum()\n",
        "print(\"Null Values:\\n\", null_values)\n"
      ],
      "metadata": {
        "id": "00gE_c9d2EYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(axis = 0)\n",
        "print(df.isnull().sum(axis = 0))"
      ],
      "metadata": {
        "id": "uiAxamPG2LwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is clean and complete, with no missing values.\n",
        "\n",
        "No imputation, row removal, or preprocessing for missing data is required.\n",
        "\n",
        "This is ideal for machine learning models such as KNN, since distance-based algorithms are sensitive to missing values."
      ],
      "metadata": {
        "id": "hWZTIOnUQ-mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Full path to the CSV\n",
        "# Corrected path to point to the Colab-accessible output file\n",
        "output_file_path = Path(r\"/content/output.csv\")\n",
        "\n",
        "# Read the CSV\n",
        "transposed_data_combined = pd.read_csv(output_file_path)\n",
        "\n",
        "# Remove the Label column\n",
        "numerical_columns = transposed_data_combined.drop(columns=['Label'])\n",
        "\n",
        "# Calculate the Euclidean distance matrix\n",
        "distance_matrix = np.sqrt(\n",
        "    np.maximum(0.0,\n",
        "        (np.abs(numerical_columns.values)**2).sum(axis=1)[:, None] +\n",
        "        (np.abs(numerical_columns.values)**2).sum(axis=1)[None, :] -\n",
        "        2 * np.abs(numerical_columns.values).dot(np.abs(numerical_columns.values).T)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_distance_matrix = pd.DataFrame(\n",
        "    distance_matrix,\n",
        "    index=numerical_columns.index,\n",
        "    columns=numerical_columns.index\n",
        ")\n",
        "\n",
        "# Print distance matrix\n",
        "print(df_distance_matrix)\n",
        "\n",
        "# Save distance matrix to CSV\n",
        "distance_file_path = output_file_path.parent / \"distance_matrix.csv\"\n",
        "df_distance_matrix.to_csv(distance_file_path, index=False)\n",
        "\n",
        "print(\"Saved distance matrix to:\", distance_file_path)\n"
      ],
      "metadata": {
        "id": "o4IO52rm2M1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distance matrix contains pairwise Euclidean distances between all data samples. Zero values along the diagonal indicate self-distance, while non-zero values represent similarity differences between samples. This matrix forms the basis for neighbor selection in the K-NN classification algorithm."
      ],
      "metadata": {
        "id": "ILIZhBKHRU7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Example usage\n",
        "X = np.random.rand(100, 5)\n",
        "y = np.random.randint(0, 2, 100)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "vFTiisO02YMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has been split into training and testing sets.\n",
        "\n",
        "Training set shape (80, 5): 80 samples used to train the model, each with 5 features.\n",
        "\n",
        "Testing set shape (20, 5): 20 samples reserved for evaluating model performance on unseen data.\n",
        "\n",
        "This split helps assess how well the model generalizes and reduces the risk of overfitting."
      ],
      "metadata": {
        "id": "ugk8qzFSRso2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KNNClassifier:\n",
        "   def __init__(self, k=3):\n",
        "       self.k = k\n",
        "       self.X_train = None\n",
        "       self.y_train = None\n",
        "   def fit(self, X_train, y_train):\n",
        "       self.X_train = X_train\n",
        "       self.y_train = y_train\n",
        "   def predict(self, X_test):\n",
        "       predictions = []\n",
        "       for x_test_instance in X_test:\n",
        "           distances = [self.euclidean_distance(x_test_instance,\n",
        "   x_train_instance) for x_train_instance in self.X_train]\n",
        "           nearest_indices = np.argsort(distances)[:self.k]\n",
        "           nearest_labels = [self.y_train[idx] for idx in nearest_indices]\n",
        "           predicted_label = self.most_common(nearest_labels)\n",
        "           predictions.append(predicted_label)\n",
        "       return predictions\n",
        "   def euclidean_distance(self, x1, x2):\n",
        "       return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "   def most_common(self, lst):\n",
        "       unique, counts = np.unique(lst, return_counts=True)\n",
        "       index = np.argmax(counts)\n",
        "       return unique[index]\n",
        "# Assuming df is your DataFrame\n",
        "X = df.iloc[:, :-1].values # Assuming the last column is the target variable\n",
        "y = df.iloc[:, -1].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "  random_state=42)\n",
        "knn_classifier = KNNClassifier(k=5)\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "# Make predictions on the test set\n",
        "predictions = knn_classifier.predict(X_test)\n",
        "# Display the predictions\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "UBtKiEli2bsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "090e979e"
      },
      "source": [
        "# Re-read the output.csv to ensure df is updated with all labels\n",
        "output_file_path = Path(r\"/content/output.csv\")\n",
        "df = pd.read_csv(output_file_path)\n",
        "\n",
        "# Print unique labels and their counts from the 'Label' column\n",
        "print(\"Unique labels and their counts in the dataset:\")\n",
        "print(df['Label'].value_counts())\n",
        "\n",
        "print(\"\\nTotal number of unique labels:\", len(df['Label'].unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "def hold_out_split(X, y, test_size=0.2, random_seed=None):\n",
        "    if random_seed:\n",
        "       random.seed(random_seed)\n",
        "    data = list(zip(X, y))\n",
        "    random.shuffle(data)\n",
        "    split_index = int(len(data) * (1 - test_size))\n",
        "    train_data, test_data = data[:split_index], data[split_index:]\n",
        "    X_train, y_train = zip(*train_data)\n",
        "    X_test, y_test = zip(*test_data)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "# Usage\n",
        "X_train, X_test, y_train, y_test = hold_out_split(X, y, test_size=0.2,random_seed=42)\n",
        "knn_classifier = KNNClassifier(k=5)\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "# Make predictions on the test set\n",
        "predictions = knn_classifier.predict(X_test)\n",
        "# Display the predictions\n",
        "print(len(predictions))\n",
        "print(len(y_test))\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "Ss6ehTlS2eUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score,f1_score, confusion_matrix\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "# Calculate recall\n",
        "recall = recall_score(y_test, predictions,average='weighted')\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test, predictions,average='weighted')\n",
        "# Calculate F1 Score\n",
        "f1 = f1_score(y_test, predictions,average='weighted')\n",
        "print(f\"Accuracy: {accuracy:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}, F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "W1BhEE3u2hIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y\n",
        "import pandas as pd\n",
        "\n",
        "duplicates = pd.merge(pd.DataFrame(X_train), pd.DataFrame(X_test), how='inner')\n",
        "print(\"Duplicate rows between train and test:\", len(duplicates))\n",
        "\n"
      ],
      "metadata": {
        "id": "B3bJF0nt2iSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "xX1Jrj1i2mcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique labels in y:\", np.unique(y))\n"
      ],
      "metadata": {
        "id": "NXNjmnkl2oEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "#y"
      ],
      "metadata": {
        "id": "maIsQu2Y2qdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_k_fold_validation(X, y, k=2):\n",
        "    data = list(zip(X, y))\n",
        "    np.random.shuffle(data)\n",
        "    fold_size = len(data) // k\n",
        "    folds = [data[i:i + fold_size] for i in range(0, len(data), fold_size)]\n",
        "    for i in range(k):\n",
        "        test_data = folds[i]\n",
        "        train_data = [item for j in range(k) if j != i for item in folds[j]]\n",
        "        X_train, y_train = zip(*train_data)\n",
        "        X_test, y_test = zip(*test_data)\n",
        "        yield np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)\n",
        "# Example usage:\n",
        "accuracies = []\n",
        "for X_train, X_test, y_train, y_test in custom_k_fold_validation(X, y,2):\n",
        "    knn_classifier = KNNClassifier(k=5)\n",
        "    knn_classifier.fit(X_train, y_train)\n",
        "# Make predictions on the test set\n",
        "    predictions = knn_classifier.predict(X_test)\n",
        "# Display the predictions\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    accuracies.append(accuracy)\n",
        "    print(accuracy)\n",
        "overall_accuracy = np.mean(accuracies)\n",
        "print(f\"Overall Accuracy: {overall_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "0-8-C1gt2sXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the range of K values to try\n",
        "k_values = range(1, 20, 2)\n",
        "\n",
        "# Lists to store cross-validation and test accuracies\n",
        "cv_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Perform grid search\n",
        "for k in k_values:\n",
        "    # Lists to store fold accuracies during cross-validation\n",
        "    fold_accuracies = []\n",
        "    print(\"Evaluating k =\", k)\n",
        "\n",
        "    # Cross-validation loop\n",
        "    for X_train_fold, X_test_fold, y_train_fold, y_test_fold in custom_k_fold_validation(X_train, y_train, k=2):\n",
        "        # Initialize KNN classifier\n",
        "        knn_classifier = KNNClassifier(k=k)\n",
        "        knn_classifier.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "        # Make predictions on the fold test set\n",
        "        predictions_fold = knn_classifier.predict(X_test_fold)\n",
        "\n",
        "        # Calculate accuracy for this fold\n",
        "        fold_accuracy = accuracy_score(y_test_fold, predictions_fold)\n",
        "        print(\"Fold accuracy:\", fold_accuracy)\n",
        "        fold_accuracies.append(fold_accuracy)\n",
        "\n",
        "    # Calculate mean cross-validation accuracy for this K\n",
        "    cv_accuracy = np.mean(fold_accuracies)\n",
        "    cv_accuracies.append(cv_accuracy)\n",
        "\n",
        "    # Train the model on the entire training set with the current K\n",
        "    knn_classifier = KNNClassifier(k=k)\n",
        "    knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    predictions_test = knn_classifier.predict(X_test)\n",
        "\n",
        "    # Calculate test accuracy for this K\n",
        "    test_accuracy = accuracy_score(y_test, predictions_test)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "# Find the best K\n",
        "best_k = k_values[np.argmax(cv_accuracies)]\n",
        "best_accuracy = cv_accuracies[np.argmax(cv_accuracies)]\n",
        "print(f\"Best K: {best_k}, Cross-validation Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(k_values, cv_accuracies, label='Cross-validation Accuracy')\n",
        "plt.plot(k_values, test_accuracies, label='Test Accuracy')\n",
        "plt.scatter([best_k], [best_accuracy], color='red', marker='o', label=f'Best K={best_k}')\n",
        "plt.xlabel('K Value')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-8vPrLHk2ujp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "X = df.iloc[:, :-1].values  # Features\n",
        "labels = df.iloc[:, -1].values\n",
        "\n",
        "# Handle any potential NaNs in X before standardization/PCA\n",
        "# Re-apply dropna just in case df was loaded uncleanly elsewhere.\n",
        "df_cleaned = df.dropna(axis=0) # Create a cleaned version\n",
        "X_cleaned = df_cleaned.iloc[:, :-1].values\n",
        "labels_cleaned = df_cleaned.iloc[:, -1].values\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_standardized = scaler.fit_transform(X_cleaned)\n",
        "\n",
        "# Determine max possible components\n",
        "max_components = min(X_standardized.shape)  # min(n_samples, n_features)\n",
        "n_components = min(400, max_components)    # adjust automatically\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X_standardized)\n",
        "\n",
        "# Display the result\n",
        "print(\"Original shape:\", X_cleaned.shape)\n",
        "print(\"PCA transformed shape:\", X_pca.shape)\n"
      ],
      "metadata": {
        "id": "PnFPpioe2xCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Use labels_cleaned which corresponds to the cleaned X_pca data\n",
        "labels_pca = pd.DataFrame(labels_cleaned)\n",
        "df = pd.DataFrame(X_pca)\n",
        "df = pd.concat([df, labels_pca], axis=1)\n",
        "df"
      ],
      "metadata": {
        "id": "FI3E26lN21Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "class KNNClassifier:\n",
        "\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "        self.model = KNeighborsClassifier(n_neighbors=self.k)\n",
        "    def fit(self, X, y):\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "    def score(self, X, y):\n",
        "        return self.model.score(X, y)\n",
        "    def euclidean_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "    def most_common(self, lst):\n",
        "        unique, counts = np.unique(lst, return_counts=True)\n",
        "        index = np.argmax(counts)\n",
        "        return unique[index]\n",
        "# Assuming df is your DataFrame\n",
        "# Ensure df is cleaned of any NaNs before splitting for the classifier\n",
        "df_cleaned_for_knn = df.dropna(axis=0)\n",
        "X = df_cleaned_for_knn.iloc[:, :-1].values # Assuming the last column is the target variable\n",
        "y = df_cleaned_for_knn.iloc[:, -1].values\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                random_state=42)\n",
        "knn_classifier = KNNClassifier(k=5)\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "# Make predictions on the test set\n",
        "predictions = knn_classifier.predict(X_test)\n",
        "# Display the predictions\n",
        "print(\"Number of predictions:\", len(predictions))\n",
        "print(\"Number of test samples:\", len(X_test))\n",
        "print(\"Predictions:\", predictions)\n",
        "# Calculate and display accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "GMl9njdP23Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = hold_out_split(X, y, test_size=0.2,\n",
        "                                                random_seed=42)\n",
        "knn_classifier = KNNClassifier(k=5)\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "# Make predictions on the test set\n",
        "predictions = knn_classifier.predict(X_test)\n",
        "# Display the predictions\n",
        "print(len(predictions))\n",
        "print(len(y_test))\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "QkczidmI25OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = []\n",
        "for X_train, X_test, y_train, y_test in custom_k_fold_validation(X, y,2):\n",
        "    knn_classifier = KNNClassifier(k=5)\n",
        "    knn_classifier.fit(X_train, y_train)\n",
        "# Make predictions on the test set\n",
        "    predictions = knn_classifier.predict(X_test)\n",
        "# Display the predictions\n",
        "# print(predictions)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    accuracies.append(accuracy)\n",
        "    print(accuracy)\n",
        "overall_accuracy = np.mean(accuracies)\n",
        "print(f\"Overall Accuracy: {overall_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "IVyt0crR27qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score,f1_score, confusion_matrix\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "# Calculate recall\n",
        "recall = recall_score(y_test, predictions,average='weighted')\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test, predictions,average='weighted')\n",
        "# Calculate F1 Score\n",
        "f1 = f1_score(y_test, predictions,average='weighted')\n",
        "print(f\"Accuracy: {accuracy:.4f}, Recall: {recall:.4f}, Precision: {precision:4f}, F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "D284BCIR29jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Redefine KNNClassifier to be scikit-learn compatible\n",
        "class KNNClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "        # Initialize the scikit-learn KNeighborsClassifier here\n",
        "        self.model = KNeighborsClassifier(n_neighbors=self.k)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return self.model.score(X, y)\n",
        "\n",
        "    # These methods are from the original custom class, but not strictly needed\n",
        "    # for sklearn compatibility for cross_val_score.\n",
        "    def euclidean_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "    def most_common(self, lst):\n",
        "        unique, counts = np.unique(lst, return_counts=True)\n",
        "        index = np.argmax(counts)\n",
        "        return unique[index]\n",
        "\n",
        "# Define the range of K values to try\n",
        "k_values = range(1, 3)\n",
        "# Lists to store cross-validation and test accuracies\n",
        "cv_accuracies = []\n",
        "test_accuracies = []\n",
        "# Perform grid search\n",
        "for k in k_values:\n",
        "# Create a KNN classifier with the current K\n",
        "   knn_classifier = KNNClassifier(k=k)\n",
        "# Cross-validation using scikit-learn's KFold\n",
        "   kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
        "   fold_accuracies = cross_val_score(knn_classifier, X_train, y_train, cv=kf,scoring='accuracy')\n",
        "# Calculate mean cross-validation accuracy for this K\n",
        "   cv_accuracy = np.mean(fold_accuracies)\n",
        "   cv_accuracies.append(cv_accuracy)\n",
        "\n",
        "# Fit the model on the entire training set with the current K\n",
        "   knn_classifier.fit(X_train, y_train)\n",
        "# Make predictions on the test set\n",
        "   predictions_test = knn_classifier.predict(X_test)\n",
        "# Calculate test accuracy for this K\n",
        "   test_accuracy = accuracy_score(y_test, predictions_test)\n",
        "   test_accuracies.append(test_accuracy)\n",
        "best_k = k_values[np.argmax(cv_accuracies)]\n",
        "# Plot the results\n",
        "plt.plot(k_values, cv_accuracies, label='Cross-validation Accuracy')\n",
        "plt.plot(k_values, test_accuracies, label='Test Accuracy')\n",
        "plt.scatter(best_k, max(cv_accuracies), color='red', marker='*', label=f'Best K = {best_k}')\n",
        "plt.xlabel('K Value')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0TGJR0dX2_X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef338c22"
      },
      "source": [
        "### Question 2: K-NN with PCA-transformed data\n",
        "\n",
        "Now, we will apply the K-NN algorithm to the dataset after applying Principal Component Analysis (PCA) for dimensionality reduction. We will repeat all the steps from Question 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f45baa2"
      },
      "source": [
        "# Re-extract features (X) and labels (y) from the PCA-transformed DataFrame\n",
        "X_pca_transformed = df.iloc[:, :-1].values # All columns except the last one as features\n",
        "y_pca_transformed = df.iloc[:, -1].values  # The last column as labels\n",
        "\n",
        "print(\"Shape of X after PCA:\", X_pca_transformed.shape)\n",
        "print(\"Unique labels after PCA:\", np.unique(y_pca_transformed))\n",
        "\n",
        "# Split data into training and testing sets using the PCA-transformed data\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(\n",
        "    X_pca_transformed, y_pca_transformed, test_size=0.2, random_state=42, stratify=y_pca_transformed\n",
        ")\n",
        "\n",
        "print(\"Train shape (PCA):\", X_train_pca.shape, \"Test shape (PCA):\", X_test_pca.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29814b87"
      },
      "source": [
        "#### K-NN Model Implementation (without built-in libraries) on PCA data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b45a6d9"
      },
      "source": [
        "# Initialize and train the custom KNN classifier with PCA data\n",
        "knn_classifier_pca = KNNClassifier(k=5) # Using k=5 as an initial value\n",
        "knn_classifier_pca.fit(X_train_pca, y_train_pca)\n",
        "\n",
        "# Make predictions on the test set (PCA data)\n",
        "predictions_pca = knn_classifier_pca.predict(X_test_pca)\n",
        "\n",
        "# Calculate and display accuracy for PCA data\n",
        "accuracy_pca = accuracy_score(y_test_pca, predictions_pca)\n",
        "print(f\"Accuracy with custom KNN (PCA data): {accuracy_pca * 100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d18e1c3f"
      },
      "source": [
        "#### Apply hold-out validation on PCA data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc438d90"
      },
      "source": [
        "# Apply hold-out split with PCA data\n",
        "X_train_holdout_pca, X_test_holdout_pca, y_train_holdout_pca, y_test_holdout_pca = hold_out_split(\n",
        "    X_pca_transformed, y_pca_transformed, test_size=0.2, random_seed=42\n",
        ")\n",
        "\n",
        "# Initialize and train KNN classifier with hold-out split PCA data\n",
        "knn_classifier_holdout_pca = KNNClassifier(k=5)\n",
        "knn_classifier_holdout_pca.fit(X_train_holdout_pca, y_train_holdout_pca)\n",
        "\n",
        "# Make predictions on the hold-out test set (PCA data)\n",
        "predictions_holdout_pca = knn_classifier_holdout_pca.predict(X_test_holdout_pca)\n",
        "\n",
        "# Calculate and display accuracy for hold-out split PCA data\n",
        "accuracy_holdout_pca = accuracy_score(y_test_holdout_pca, predictions_holdout_pca)\n",
        "print(f\"Accuracy with hold-out split (PCA data): {accuracy_holdout_pca * 100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad439b8c"
      },
      "source": [
        "#### Apply custom K-Fold cross-validation on PCA data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a77050dd"
      },
      "source": [
        "accuracies_cv_pca = []\n",
        "for X_train_fold_pca, X_test_fold_pca, y_train_fold_pca, y_test_fold_pca in custom_k_fold_validation(\n",
        "    X_pca_transformed, y_pca_transformed, k=2\n",
        "):\n",
        "    knn_classifier_cv_pca = KNNClassifier(k=5)\n",
        "    knn_classifier_cv_pca.fit(X_train_fold_pca, y_train_fold_pca)\n",
        "    predictions_cv_pca = knn_classifier_cv_pca.predict(X_test_fold_pca)\n",
        "    accuracy_cv_fold_pca = accuracy_score(y_test_fold_pca, predictions_cv_pca)\n",
        "    accuracies_cv_pca.append(accuracy_cv_fold_pca)\n",
        "    print(f\"Fold Accuracy (PCA): {accuracy_cv_fold_pca}\")\n",
        "\n",
        "overall_accuracy_cv_pca = np.mean(accuracies_cv_pca)\n",
        "print(f\"Overall K-Fold Accuracy (PCA data): {overall_accuracy_cv_pca * 100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "040d654a"
      },
      "source": [
        "#### Calculate classification measures (Accuracy, Recall, Precision, F1 Score) on PCA data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78a42ff7"
      },
      "source": [
        "# Using the predictions from the initial train_test_split with PCA data\n",
        "# Calculate accuracy\n",
        "accuracy_pca_metrics = accuracy_score(y_test_pca, predictions_pca)\n",
        "# Calculate recall\n",
        "recall_pca = recall_score(y_test_pca, predictions_pca, average='weighted')\n",
        "# Calculate precision\n",
        "precision_pca = precision_score(y_test_pca, predictions_pca, average='weighted')\n",
        "# Calculate F1 Score\n",
        "f1_pca = f1_score(y_test_pca, predictions_pca, average='weighted')\n",
        "\n",
        "print(f\"Metrics for PCA data:\\nAccuracy: {accuracy_pca_metrics:.4f}, Recall: {recall_pca:.4f}, Precision: {precision_pca:.4f}, F1 Score: {f1_pca:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bee4dc76"
      },
      "source": [
        "#### Find the optimized K value and plot cross-validation and test accuracy (PCA data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2f90532"
      },
      "source": [
        "# Define the range of K values to try (adjust as needed based on insights)\n",
        "k_values_pca = range(1, 20, 2) # Example range, consider expanding if needed\n",
        "\n",
        "cv_accuracies_pca = []\n",
        "test_accuracies_pca = []\n",
        "\n",
        "for k_val in k_values_pca:\n",
        "    fold_accuracies_grid_pca = []\n",
        "    print(f\"Evaluating k = {k_val} (PCA data)\")\n",
        "\n",
        "    for X_train_fold_grid_pca, X_test_fold_grid_pca, y_train_fold_grid_pca, y_test_fold_grid_pca in custom_k_fold_validation(\n",
        "        X_train_pca, y_train_pca, k=2 # Using X_train_pca for CV as per original problem setup\n",
        "    ):\n",
        "        knn_classifier_grid_pca = KNNClassifier(k=k_val)\n",
        "        knn_classifier_grid_pca.fit(X_train_fold_grid_pca, y_train_fold_grid_pca)\n",
        "        predictions_fold_grid_pca = knn_classifier_grid_pca.predict(X_test_fold_grid_pca)\n",
        "        fold_accuracy_grid_pca = accuracy_score(y_test_fold_grid_pca, predictions_fold_grid_pca)\n",
        "        fold_accuracies_grid_pca.append(fold_accuracy_grid_pca)\n",
        "\n",
        "    cv_accuracy_mean_pca = np.mean(fold_accuracies_grid_pca)\n",
        "    cv_accuracies_pca.append(cv_accuracy_mean_pca)\n",
        "\n",
        "    knn_classifier_test_pca = KNNClassifier(k=k_val)\n",
        "    knn_classifier_test_pca.fit(X_train_pca, y_train_pca)\n",
        "    predictions_test_grid_pca = knn_classifier_test_pca.predict(X_test_pca)\n",
        "    test_accuracy_grid_pca = accuracy_score(y_test_pca, predictions_test_grid_pca)\n",
        "    test_accuracies_pca.append(test_accuracy_grid_pca)\n",
        "\n",
        "best_k_pca = k_values_pca[np.argmax(cv_accuracies_pca)]\n",
        "best_accuracy_pca = cv_accuracies_pca[np.argmax(cv_accuracies_pca)]\n",
        "print(f\"Best K (PCA data): {best_k_pca}, Cross-validation Accuracy: {best_accuracy_pca:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values_pca, cv_accuracies_pca, label='Cross-validation Accuracy (PCA)')\n",
        "plt.plot(k_values_pca, test_accuracies_pca, label='Test Accuracy (PCA)')\n",
        "plt.scatter([best_k_pca], [best_accuracy_pca], color='red', marker='o', label=f'Best K (PCA)={best_k_pca}')\n",
        "plt.xlabel('K Value')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('K-NN Accuracy vs. K Value (PCA-transformed Data)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question#3**"
      ],
      "metadata": {
        "id": "WU4EsYiAPjqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which model gives the best accuracy and why?\n",
        "Using PCA reduced the number of features from 1000 to 400, which made the computations faster\n",
        "and less complex. While the accuracy only improved slightly (by about 0.88%), reducing the\n",
        "number of features helped avoid the curse of dimensionality. This means that even with fewer\n",
        "features, the calculations were quicker, and the overall classification accuracy wasn’t significantly\n",
        "affected.\n",
        "Best Model The best-performing k-NN model used PCA and had\n",
        "k=3. This combination achieved an accuracy of 17.71%"
      ],
      "metadata": {
        "id": "AV3D6HlrPouL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why It Performed Better:**"
      ],
      "metadata": {
        "id": "ln4shDrFPrmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**• Reducing Dimensions:** PCA reduced the original 1000 features to just 400, minimizing\n",
        "the impact of the curse of dimensionality. It likely eliminated irrelevant or noisy features"
      ],
      "metadata": {
        "id": "df2WsMpYP1Rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Better Generalization:** With fewer features, the model became less prone to overfitting,\n",
        "improving its ability to perform well on unseen data."
      ],
      "metadata": {
        "id": "FsxfACY9P7Qg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Faster Computation:** Byloweringthedimensionality, PCA simplified distance calculations,\n",
        "making the classification process quicker and more efficient"
      ],
      "metadata": {
        "id": "Hp3CEvGUP_N4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Improved Class Separation:** PCA restructured the feature space to focus on the most\n",
        "significant variations, which might have made it easier to distinguish between overlapping\n",
        "classes."
      ],
      "metadata": {
        "id": "bVPNLj1oQDzu"
      }
    }
  ]
}